# -*- coding: utf-8 -*-
"""MLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zXQB3kwxZBkLqPGtQV4v78U_KYlc1N8I
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install tensorflow

import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras import layers, models
import random
import matplotlib.pyplot as plt

OUTPUT_DIR = "/content/drive/My Drive/preprocessing_mesh_data"

EMOTION_CATEGORIES = ['슬픔', '기쁨', '분노', '불안', '당황', '상처', '중립']

all_meshes = []
all_labels = []

for emotion in EMOTION_CATEGORIES:
    mesh_path = os.path.join(OUTPUT_DIR, f"meshes_{emotion}.npy")
    label_path = os.path.join(OUTPUT_DIR, f"labels_{emotion}.npy")

    meshes = np.load(mesh_path)
    labels = np.load(label_path)

    all_meshes.append(meshes)
    all_labels.append(labels)

X = np.concatenate(all_meshes, axis=0)
y = np.concatenate(all_labels, axis=0)

print("원본 X shape:", X.shape)
print("원본 y shape:", y.shape)

# 1) flatten
N, H, C = X.shape
X = X.reshape(N, H * C)
print("flatten 후 X shape:", X.shape)

# 2) 셔플
X, y = shuffle(X, y, random_state=42)

# 3) train / val split
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Train:", X_train.shape, y_train.shape)
print("Val  :", X_val.shape, y_val.shape)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled   = scaler.transform(X_val)

print(X_train_scaled.shape, X_val_scaled.shape)

SEED = 42
tf.random.set_seed(SEED)
np.random.seed(SEED)
random.seed(SEED)

INPUT_DIM = X_train.shape[1]
NUM_CLASSES = 7

model = models.Sequential([
    layers.Input(shape=(INPUT_DIM,)),
    layers.Dense(512, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(NUM_CLASSES, activation='softmax'),
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-4),
    loss='sparse_categorical_crossentropy',  # y가 0~6 정수라서 sparse 사용
    metrics=['accuracy'],
)

model.summary()


checkpoint = tf.keras.callbacks.ModelCheckpoint(
    "best_mlp.keras",
    monitor="val_loss",
    save_best_only=True,
    save_weights_only=False
)

callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=5,
        restore_best_weights=True
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.5,
        patience=3,
        min_lr=1e-6,
        verbose=1
    ),
    checkpoint
]

history = model.fit(
    X_train_scaled, y_train,
    batch_size=128,
    epochs=100,
    validation_data=(X_val_scaled, y_val),
    callbacks=callbacks
)

print(history.history.keys())

epochs = range(1, len(history.history['loss']) + 1)

# 1) Loss
plt.figure(figsize=(8, 5))
plt.plot(epochs, history.history['loss'], label='Train Loss')
plt.plot(epochs, history.history['val_loss'], label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training & Validation Loss')
plt.legend()
plt.grid(True)
plt.show()

# 2) Accuracy
plt.figure(figsize=(8, 5))
plt.plot(epochs, history.history['accuracy'], label='Train Accuracy')
plt.plot(epochs, history.history['val_accuracy'], label='Val Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training & Validation Accuracy')
plt.legend()
plt.grid(True)
plt.show()

